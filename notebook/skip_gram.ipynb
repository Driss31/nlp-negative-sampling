{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # Used to split text into sentences\n",
    "import string  # Used to get all punctuation\n",
    "import pickle  # Used to save and load embeddings\n",
    "import logging  # Used to save steps in a text file instead of printing them\n",
    "import tqdm  # Used to time the training\n",
    "from scipy.special import expit  # Used to compute the gradient\n",
    "\n",
    "\n",
    "__authors__ = ['Driss Debbagh-Nour','Mehdi Mikou','Soufiane Hadji', 'Mohamed Aymane Benayada']\n",
    "__emails__  = ['driss.debbagh-nour@student.ecp.fr','mehdi.mikou@student.ecp.fr',\n",
    "               'soufiane.hadji@student.ecp.fr', \"mohamed-aymane.benayada@student.ecp.fr\"]\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='test_log.log',level=logging.INFO,\\\n",
    "      format='%(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s')\n",
    "\n",
    "\n",
    "def concat_text(file):\n",
    "    '''\n",
    "    Used for documents which contain a long text rather than multiple sentences.\n",
    "    The functions concatenate the whole text and removes quotes\n",
    "    '''\n",
    "    all_text_list = []\n",
    "    with open(file) as f:\n",
    "        for l in f:\n",
    "            l = l[:-1] #We remove the line jumping\n",
    "            all_text_list.append(l)\n",
    "    concat_text = ''.join(all_text_list)\n",
    "    concat_text = concat_text.replace('\"', '')\n",
    "    concat_text = concat_text.replace(\"\\'\", '')\n",
    "    \n",
    "    return concat_text\n",
    "\n",
    "\n",
    "def text2sentences(file, only_sentences=True):\n",
    "    \"\"\"\n",
    "    Split words while removing all punctuation, stopwords and non-alpha words while keeping contractions together\n",
    "    2 types of input: \n",
    "    - If it's a whole text: Split text into sentences using punctuation (. ? ! ;) after concatenating all text\n",
    "    - If it's a bunch of sentences: Split the document using \\n \n",
    "    Then split sentences by whitespace\n",
    "    \n",
    "    Returns the tokenization of our file\n",
    "    \"\"\"\n",
    "    \n",
    "    final_sentences = []\n",
    "    \n",
    "    # Load stop words\n",
    "    #stop_words = stopwords.words('english')\n",
    "    #stop_words.append(\"\")\n",
    "    \n",
    "    # Differentiate the two types of documents\n",
    "    if not only_sentences:\n",
    "        concatenated_file = concat_text(file)\n",
    "        sentences = re.split(r'[.?!;]', concatenated_file)\n",
    "    else:\n",
    "        sentences = [line.rstrip('\\n') for line in open(file, encoding=\"utf8\")]\n",
    "    \n",
    "    # Tokenize our document\n",
    "    for sentence in sentences:\n",
    "        words_list = sentence.lower().split()\n",
    "        ponctuation_remover = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(ponctuation_remover) for w in words_list \n",
    "                    if w not in string.punctuation]\n",
    "        final_sentence = [word for word in stripped if word.isalpha()]\n",
    "        final_sentences.append(final_sentence)\n",
    "\n",
    "    return final_sentences\n",
    "\n",
    "\n",
    "def count_words(list_words):\n",
    "    \"\"\"\n",
    "    Take a list of words and return a dictionary with words as keys and their occurence as values\n",
    "    \"\"\"\n",
    "    dict_occurences = {}\n",
    "    for word in list_words:\n",
    "        try:\n",
    "            dict_occurences[word] += 1\n",
    "        except KeyError:\n",
    "            dict_occurences[word] = 1\n",
    "    return dict_occurences\n",
    "            \n",
    "    \n",
    "def rare_word_pruning(sentences, min_count):\n",
    "    \"\"\"\n",
    "    Remove words that occures less than min_count time\n",
    "    \"\"\"\n",
    "    words = [word for sentence in sentences for word in sentence]\n",
    "    dict_occurences = count_words(words)\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            if dict_occurences[word] >= min_count:\n",
    "                new_sentence.append(word)\n",
    "        new_sentences.append(new_sentence)\n",
    "    return new_sentences\n",
    "\n",
    "        \n",
    "# def high_and_low_frequency_pruning(sentences, min_count, max_count_ratio):\n",
    "#     \"\"\"\n",
    "#     Remove words that occures less than min_count and more than max_count times\n",
    "#     \"\"\"\n",
    "#     words = [word for sentence in sentences for word in sentence]\n",
    "#     max_count = int(len(words) * max_count_ratio)\n",
    "#     dict_occurences = count_words(words)\n",
    "#     new_sentences = []\n",
    "#     for sentence in sentences:\n",
    "#         new_sentence = []\n",
    "#         for word in sentence:\n",
    "#             if min_count<= dict_occurences[word] <= max_count:\n",
    "#                 new_sentence.append(word)\n",
    "#         new_sentences.append(new_sentence)\n",
    "#     return new_sentences\n",
    "\n",
    "\n",
    "def get_positive_pairs(processed_sentences, winSize):\n",
    "    \"\"\"\n",
    "    Get Pairs of words that co-occur (in the window delimited by winSize): Positive examples\n",
    "    \"\"\"\n",
    "    # List of positive pairs to return\n",
    "    positive_pairs = []\n",
    "    \n",
    "    # Initialize dictionaries that will allow to move from strings to their indexes\n",
    "    words_voc = {}  \n",
    "    context_voc = {}\n",
    "    \n",
    "    # Initialize indexes for words and contexts\n",
    "    indexer_words = 0\n",
    "    indexer_context = 0\n",
    "    \n",
    "    for sentence in processed_sentences:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in words_voc:\n",
    "                words_voc[word] = indexer_words\n",
    "                indexer_words += 1\n",
    "            \n",
    "            for j in range(max(0, i - winSize//2), min(i + winSize//2 + 1, len(sentence))):  # Be careful of edges\n",
    "                if i != j:  # word != context\n",
    "                    context = sentence[j]\n",
    "                    if context not in context_voc:\n",
    "                        context_voc[context] = indexer_context\n",
    "                        indexer_context += 1\n",
    "                    positive_pairs.append((words_voc[word], context_voc[context]))\n",
    "                    \n",
    "    return positive_pairs, words_voc, context_voc\n",
    "\n",
    "\n",
    "def get_negative_pairs(positive_pairs, negativeRate):\n",
    "    \"\"\"\n",
    "    Get Pairs of words that don't co-occur: Negative examples \n",
    "    Size: negativeRate * size(positive_pairs)\n",
    "    \"\"\"\n",
    "    # List of negative pairs to return\n",
    "    negative_pairs = []\n",
    "    nbr_positive_pairs = len(positive_pairs)\n",
    "    \n",
    "    for p_pair in positive_pairs:\n",
    "        word_index = p_pair[0]  # target word\n",
    "        for _ in range(negativeRate):\n",
    "            pair_index = np.random.randint(nbr_positive_pairs)  # Random index for a pair (can be improved)\n",
    "            negative_context = positive_pairs[pair_index][1]  # Get the random pair context index\n",
    "            negative_pairs.append((word_index, negative_context))\n",
    "            \n",
    "    return negative_pairs\n",
    "\n",
    "\n",
    "def gradient(theta, nEmbed, positive_pairs, negative_pairs, nb_words, nb_contexts):\n",
    "    \"\"\"\n",
    "    Compute gradient at init_theta for positive and negative pairs\n",
    "    \"\"\"\n",
    "    # Initialize gradient\n",
    "    grad = np.zeros(len(theta))\n",
    "    \n",
    "    # Embedding matrix of target words\n",
    "    words_matrix = theta[: nEmbed * nb_words].reshape(nb_words, nEmbed)\n",
    "    \n",
    "    # Embedding matrix of contextes\n",
    "    contexts_matrix = theta[nEmbed * nb_words:].reshape(nb_contexts, nEmbed)\n",
    "    \n",
    "    logging.info(\"Compute gradient\")\n",
    "    \n",
    "    # Positive pairs\n",
    "    logging.info(\"Positive pairs...\")\n",
    "    for p_pair in positive_pairs:\n",
    "        \n",
    "        # Get indexes of (word, context)\n",
    "        word_index = p_pair[0]\n",
    "        context_index = p_pair[1]\n",
    "        \n",
    "        # Get the actual embedding of the word and its context\n",
    "        word = words_matrix[word_index]\n",
    "        context = contexts_matrix[context_index]\n",
    "\n",
    "        # We compute the derivative of the formula given by 'Yoav Goldberg' and 'Omer Levy'\n",
    "        df_word = context * expit(-word.dot(context))\n",
    "        df_context = word * expit(-word.dot(context))\n",
    "        \n",
    "        # We actualize the gradient of the word and its context\n",
    "        grad[word_index * nEmbed: (word_index + 1) * nEmbed] += df_word\n",
    "        grad[(nb_words + context_index) * nEmbed: (nb_words + context_index + 1) * nEmbed] += df_context\n",
    "    logging.info(\"Done\")\n",
    "    \n",
    "    # Negative pairs\n",
    "    logging.info(\"Negative pairs...\")\n",
    "    for n_pair in negative_pairs:\n",
    "        \n",
    "        # Get indexes of (word, negative context)\n",
    "        word_index = n_pair[0]\n",
    "        context_index = n_pair[1]\n",
    "        \n",
    "        # Get the actual embedding of the word and its context\n",
    "        word = words_matrix[word_index]\n",
    "        context = contexts_matrix[context_index]\n",
    "        \n",
    "        # We compute the derivative of the formula given by 'Yoav Goldberg' and 'Omer Levy'\n",
    "        df_word = -context * expit(word.dot(context))\n",
    "        df_context = -word * expit(word.dot(context))\n",
    "        \n",
    "        # We actualize the gradient of the word and its negative context\n",
    "        grad[word_index * nEmbed: (word_index + 1) * nEmbed] += df_word\n",
    "        grad[(nb_words + context_index) * nEmbed: (nb_words + context_index + 1) * nEmbed] += df_context\n",
    "    logging.info(\"Done\")\n",
    "    \n",
    "    return grad\n",
    "\n",
    "        \n",
    "def loadPairs(path):\n",
    "    data = pd.read_csv(path, delimiter='\\t')\n",
    "    pairs = zip(data['word1'], data['word2'], data['similarity'])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, sentences, nEmbed=100, negativeRate=5, winSize=7, minCount=5):\n",
    "        # Preprocessing the sentences\n",
    "        # Remove rare words\n",
    "        print(\"1. Processing sentences...\")\n",
    "        processed_sentences= rare_word_pruning(sentences, minCount)\n",
    "        print(\"Done\\n\")\n",
    "        \n",
    "        # Generate positive and negative pairs\n",
    "        print(\"2. Generating positive pairs...\")\n",
    "        self.positive_pairs, self.words_voc, self.context_voc = get_positive_pairs(processed_sentences, winSize)\n",
    "        print(\"Done\\n\")\n",
    "        \n",
    "        print(\"3. Generating negative samples...\")\n",
    "        # Generate negative samples\n",
    "        self.negative_pairs = get_negative_pairs(self.positive_pairs, negativeRate)\n",
    "        print(\"Done\")\n",
    "\n",
    "    def train(self, learning_rate=0.01, epochs=5, batchsize=500, nEmbed=100, negativeRate=5):\n",
    "        \"\"\"Create W matrix containing the embeddings of all words\"\"\"\n",
    "        \n",
    "        # Get nb of words, nb of contexts, nb of pairs\n",
    "        nb_words = len(list(self.words_voc.keys()))\n",
    "        nb_contexts = len(list(self.context_voc.keys()))\n",
    "        nb_pairs = len(self.positive_pairs)\n",
    "  \n",
    "        # Initialize theta: vector of parameters\n",
    "        nb_param = nEmbed * (nb_words + nb_contexts)  # Number of parameters\n",
    "        theta = np.random.random(nb_param) * 1e-5\n",
    "\n",
    "        # Compute Stochastic Gradient\n",
    "        print(\"TRAINING: epochs: {}, learning_rate: {}, batch size: {}\".format(epochs, learning_rate, batchsize))\n",
    "        logging.info(\"TRAINING: epochs: {}, learning_rate: {}, batch size: {}\".format(epochs, learning_rate, batchsize))\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "            logging.info(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "            \n",
    "            # We update theta after computing the gradient of each batch (which size is batchsize)\n",
    "            for batch_number in tqdm.tqdm(range(nb_pairs // batchsize)):\n",
    "                logging.info(\"batch_number {}/{}\".format(batch_number+1, nb_pairs // batchsize))\n",
    "                batch_begin = batch_number * batchsize\n",
    "                batch_end = min((batch_number + 1) * batchsize, nb_pairs)\n",
    "                batch_positive = self.positive_pairs[batch_begin: batch_end]\n",
    "                batch_negative = self.negative_pairs[negativeRate * batch_begin: negativeRate * batch_end]\n",
    "                \n",
    "                # Compute the gradient at theta\n",
    "                grad = gradient(theta, nEmbed, batch_positive, batch_negative, nb_words, nb_contexts)\n",
    "                \n",
    "                # Actualize theta (since we want to maximize the 'loss', we add grad)\n",
    "                theta = theta + learning_rate*grad\n",
    "                \n",
    "            logging.info(theta)\n",
    "\n",
    "        self.theta = theta\n",
    "        \n",
    "        # Matrix of embeddings\n",
    "        self.W = theta[:nEmbed * nb_words].reshape(nb_words, nEmbed)\n",
    "\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save in binary Theta and W\"\"\"\n",
    "        with open(path + '\\\\theta', 'wb') as fichier:\n",
    "            mon_pickler = pickle.Pickler(fichier)\n",
    "            mon_pickler.dump(self.theta)\n",
    "        with open(path + '\\\\W', 'wb') as fichier:\n",
    "            mon_pickler = pickle.Pickler(fichier)\n",
    "            mon_pickler.dump(self.W)        \n",
    "        with open(path + '\\\\words_voc', 'wb') as fichier:\n",
    "            mon_pickler = pickle.Pickler(fichier)\n",
    "            mon_pickler.dump(self.words_voc)\n",
    "            \n",
    "            \n",
    "    def similarity(self, word1, word2, words_voc, W):\n",
    "        \"\"\"\n",
    "        computes similiarity between the two words. unknown words are mapped to one common vector\n",
    "        :param word1: 1st word\n",
    "        :param word2: 2nd word\n",
    "        :param words_voc: dictionary of words kept from original document as keys and their index as value \n",
    "        :param W: matrix of embeddings\n",
    "        :return: a float \\in [0,1] indicating the similarity (the higher the more similar)\n",
    "        \"\"\"\n",
    "        nEmbed = W.shape[1]\n",
    "        \n",
    "        # For words that aren't in the training set, we considere them as the same \"OOV\" \n",
    "        # and give them the same vector with low values\n",
    "        default_embd = np.ones(nEmbed) * 0.01\n",
    "        \n",
    "        # Get word_1 vector\n",
    "        if word1 in words_voc:\n",
    "            idx_word1 = words_voc[word1]\n",
    "            embd_word1 = W[idx_word1]\n",
    "        else:\n",
    "            print(\"Out of Vocabulary:\", str(word1))\n",
    "            embd_word1 = default_embd\n",
    "        \n",
    "        # Get word_2 vector\n",
    "        if word2 in words_voc:\n",
    "            idx_word2 = words_voc[word2]\n",
    "            embd_word2 = W[idx_word2]\n",
    "        else:\n",
    "            print(\"Out of Vocabulary:\", str(word2))\n",
    "            embd_word2 = default_embd\n",
    "        \n",
    "        # Compute the cosine distance\n",
    "        return abs(embd_word1.dot(embd_word2) / (np.linalg.norm(embd_word1) * np.linalg.norm(embd_word2)))\n",
    "\n",
    "    \n",
    "    def K_most_similar(self, word, K, words_voc, W):\n",
    "        dict_words_similarity = {}\n",
    "        for elt in words_voc:\n",
    "            dict_words_similarity[elt] = self.similarity(word, elt, words_voc, W)\n",
    "            \n",
    "        ranked_similar_words = sorted(dict_words_similarity, key=dict_words_similarity.get, reverse=True)\n",
    "        print(\"Similar words for\", word, \":\")\n",
    "        for i in range(1, K + 1):\n",
    "            print(\"-\", ranked_similar_words[i], ':', dict_words_similarity[ranked_similar_words[i]])\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path + '\\\\W', 'rb') as fichier:\n",
    "            my_depickler = pickle.Unpickler(fichier)\n",
    "            W = my_depickler.load()\n",
    "        with open(path + '\\\\words_voc', 'rb') as fichier:\n",
    "            my_depickler = pickle.Unpickler(fichier)\n",
    "            words_voc = my_depickler.load()\n",
    "                  \n",
    "        return W, words_voc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--text', help=\"news.en-00001-of-00100.txt\", required=True)\n",
    "    parser.add_argument('--model', help='test', required=True)\n",
    "    parser.add_argument('--test', help='test', action='store_true')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    if not opts.test:\n",
    "        sentences = text2sentences(opts.text)\n",
    "        sg = SkipGram(sentences)\n",
    "        sg.train()\n",
    "        sg.save(opts.model)\n",
    "\n",
    "    else:\n",
    "        pairs = loadPairs(opts.text)\n",
    "        W, words_voc = SkipGram.load(opts.model)\n",
    "        \n",
    "        for a,b,_ in pairs:\n",
    "            print(sg.similarity(a,b, words_voc, W))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text2sentences(\"news.en-00001-of-00100.txt\", only_sentences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306068"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_sentences = sentences[:10000]\n",
    "len(crop_sentences)\n",
    "# print(crop_sentences[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219305"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for sent in crop_sentences:\n",
    "    for word in sent:\n",
    "        counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "monskip = SkipGram(crop_sentences, nEmbed=300, negativeRate=5, winSize=7, minCount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020658\n",
      "5103290\n",
      "4901\n",
      "4901\n"
     ]
    }
   ],
   "source": [
    "print(len(monskip.positive_pairs))\n",
    "print(len(monskip.negative_pairs))\n",
    "print(len(monskip.words_voc))\n",
    "print(len(monskip.context_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2041/2041 [07:48<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470.14s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2041/2041 [07:40<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460.41s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2041/2041 [07:38<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458.43s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2041/2041 [07:48<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468.34s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2041/2041 [08:09<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489.89s)\n"
     ]
    }
   ],
   "source": [
    "monskip.train(learning_rate=0.01, epochs=5, batchsize=500, nEmbed=300, negativeRate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "monskip.save(r\"gridsearch\\10k\\dimension_300\")\n",
    "W, words_voc = monskip.load(r\"gridsearch\\10k\\dimension_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for president :\n",
      "- chief : 0.9330720967728672\n",
      "- prime : 0.9213076605844395\n",
      "- executive : 0.9079792179962911\n",
      "- minister : 0.9070468374519342\n",
      "- obama : 0.9037875920805644\n",
      "- secretary : 0.8870692732115408\n",
      "- state : 0.8798835145992542\n",
      "- director : 0.8783563506443371\n",
      "- john : 0.8777978122809695\n",
      "- former : 0.8775287728786312\n"
     ]
    }
   ],
   "source": [
    "# print(monskip.similarity(\"obama\",\"president\"))\n",
    "# monskip.similarity(\"achilles\", \"driss\", words_voc, W)\n",
    "monskip.K_most_similar(\"president\", 10, words_voc, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9003719041046656"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monskip.similarity(\"woman\", \"man\", words_voc, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140292408030202"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monskip.similarity(\"woman\", \"girl\", words_voc, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5525813805008297"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monskip.similarity(\"woman\", \"bicycle\", words_voc, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Vocabulary: grizzly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2520814392860999"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monskip.similarity(\"woman\", \"grizzly\", words_voc, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 720/720 [02:14<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134.08s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 720/720 [02:13<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133.99s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 720/720 [02:14<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134.15s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 720/720 [02:13<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133.01s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 720/720 [02:16<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136.11s)\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2642/2642 [08:11<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491.98s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2642/2642 [08:12<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492.61s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2642/2642 [08:11<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491.86s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2642/2642 [08:13<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493.76s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2642/2642 [08:13<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493.17s)\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3205/3205 [09:56<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596.95s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3205/3205 [09:54<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594.03s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3205/3205 [09:55<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595.28s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3205/3205 [09:57<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(597.92s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3205/3205 [09:56<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596.47s)\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [02:28<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148.11s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [02:22<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142.08s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [02:24<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144.96s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [02:26<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146.96s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [02:22<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142.57s)\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "TRAINING: #epochs: 5, learning_rate: 0.01, batch size: 500\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [06:06<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366.35s)\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [06:06<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366.28s)\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [06:11<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371.32s)\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [06:00<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360.79s)\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1400/1400 [06:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373.89s)\n"
     ]
    }
   ],
   "source": [
    "winSize_range  = [3, 9, 11]\n",
    "sentences = text2sentences(\"news.en-00001-of-00100.txt\", only_sentences=True)\n",
    "crop_sentences = sentences[:10000]\n",
    "for winSize_value in winSize_range:\n",
    "    monskip = SkipGram(crop_sentences, nEmbed=100, negativeRate=5, winSize=winSize_value, minCount=5)\n",
    "    monskip.train(learning_rate=0.01, epochs=5, batchsize=500, nEmbed=100, negativeRate=5)\n",
    "    monskip.save(r\"C:\\Users\\Driss Debbagh\\Desktop\\Cours 3A\\Natural_Language_Processing\\SkipGram\\gridsearch\\10k\\winsize_\" + str(winSize_value))\n",
    "    \n",
    "negativerate_range = [2, 8]\n",
    "sentences = text2sentences(\"news.en-00001-of-00100.txt\", only_sentences=True)\n",
    "crop_sentences = sentences[:10000]\n",
    "for negativerate_value in negativerate_range:\n",
    "    monskip = SkipGram(crop_sentences, nEmbed=100, negativeRate=negativerate_value, winSize=5, minCount=5)\n",
    "    monskip.train(learning_rate=0.01, epochs=5, batchsize=500, nEmbed=100, negativeRate=negativerate_value)\n",
    "    monskip.save(r\"C:\\Users\\Driss Debbagh\\Desktop\\Cours 3A\\Natural_Language_Processing\\SkipGram\\gridsearch\\10k\\negativerate_\" + str(negativerate_value))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing each value of the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Window size: 3\n",
      "Similar words for president :\n",
      "- director : 0.9792246591856569\n",
      "- california : 0.97649384551502\n",
      "- chief : 0.9746639693673325\n",
      "- chairman : 0.9742586167911036\n",
      "- bank : 0.9715236174124944\n",
      "- america : 0.9706534470582433\n",
      "- leader : 0.96962433136895\n",
      "- london : 0.967842092831337\n",
      "- office : 0.9670202297599934\n",
      "- life : 0.9665494499288213\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Window size: 5\n",
      "Similar words for president :\n",
      "- chief : 0.9593564101602977\n",
      "- secretary : 0.9494704742934456\n",
      "- executive : 0.9461626633693925\n",
      "- prime : 0.9300461180738658\n",
      "- angeles : 0.9286220659735738\n",
      "- deputy : 0.9242985502729822\n",
      "- director : 0.9224185316975538\n",
      "- los : 0.920480357299054\n",
      "- former : 0.9204484530722087\n",
      "- minister : 0.917344428830076\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Window size: 7\n",
      "Similar words for president :\n",
      "- chief : 0.9515587931062681\n",
      "- secretary : 0.9362721462546046\n",
      "- minister : 0.9316452467893469\n",
      "- john : 0.9307247506568667\n",
      "- executive : 0.9223662271484317\n",
      "- former : 0.9156480461382349\n",
      "- richard : 0.91323711580783\n",
      "- prime : 0.911429191101922\n",
      "- los : 0.9077767557982435\n",
      "- director : 0.9071329632735705\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Window size: 9\n",
      "Similar words for president :\n",
      "- secretary : 0.9536398196220982\n",
      "- minister : 0.9454848186565535\n",
      "- chief : 0.9443178964952845\n",
      "- barack : 0.9256390658832334\n",
      "- reuters : 0.9171110484641473\n",
      "- leader : 0.9102897776536839\n",
      "- prime : 0.9059471818280127\n",
      "- angeles : 0.897086938795163\n",
      "- citing : 0.8963299512638552\n",
      "- council : 0.894398557959698\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Window size: 11\n",
      "Similar words for president :\n",
      "- chief : 0.9330770496260172\n",
      "- barack : 0.919482370750021\n",
      "- obama : 0.9086894544528102\n",
      "- john : 0.8865276605823598\n",
      "- secretary : 0.8857110721421699\n",
      "- minister : 0.8798953322672031\n",
      "- sen : 0.8712612132173619\n",
      "- mike : 0.8700930895390278\n",
      "- washington : 0.8580856195258245\n",
      "- citing : 0.8554384299637808\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winSize_range  = [3, 5, 7, 9, 11]\n",
    "for window_size in winSize_range:\n",
    "    monskip = SkipGram(crop_sentences, nEmbed=100, negativeRate=5, winSize=window_size, minCount=5)\n",
    "    W, words_voc = monskip.load(r\"C:\\Users\\Driss Debbagh\\Desktop\\Cours 3A\\Natural_Language_Processing\\SkipGram\\gridsearch\\10k\\winsize_\" + str(window_size))\n",
    "    print(\"\\nWindow size:\", window_size)\n",
    "    monskip.K_most_similar(\"president\", 10, words_voc, W)\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Negative rate: 2\n",
      "Similar words for president :\n",
      "- chief : 0.9718132956742079\n",
      "- korea : 0.9642909245524272\n",
      "- foreign : 0.9552330469471004\n",
      "- secretary : 0.954838870589087\n",
      "- executive : 0.9546695762660887\n",
      "- los : 0.9540462476995708\n",
      "- saturday : 0.9527672005514987\n",
      "- south : 0.9483294769950554\n",
      "- director : 0.9451422045123068\n",
      "- smoking : 0.9419185743388031\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Negative rate: 5\n",
      "Similar words for president :\n",
      "- chief : 0.9593564101602977\n",
      "- secretary : 0.9494704742934456\n",
      "- executive : 0.9461626633693925\n",
      "- prime : 0.9300461180738658\n",
      "- angeles : 0.9286220659735738\n",
      "- deputy : 0.9242985502729822\n",
      "- director : 0.9224185316975538\n",
      "- los : 0.920480357299054\n",
      "- former : 0.9204484530722087\n",
      "- minister : 0.917344428830076\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "1. Processing sentences...\n",
      "Done\n",
      "\n",
      "2. Generating positive pairs...\n",
      "Done\n",
      "\n",
      "3. Generating negative samples...\n",
      "Done\n",
      "\n",
      "Negative rate: 8\n",
      "Similar words for president :\n",
      "- chief : 0.9773794942428166\n",
      "- executive : 0.9635627925729485\n",
      "- secretary : 0.9526132811133343\n",
      "- angeles : 0.9411750168583449\n",
      "- minister : 0.9391909200276619\n",
      "- prime : 0.9376329559787157\n",
      "- state : 0.9321833864210928\n",
      "- los : 0.9317051456141309\n",
      "- director : 0.9307956037603728\n",
      "- south : 0.9288532287585676\n",
      "\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negativerate_range = [2, 5, 8]\n",
    "for negative_rate in negativerate_range:\n",
    "    monskip = SkipGram(crop_sentences, nEmbed=100, negativeRate=negative_rate, winSize=window_size, minCount=5)\n",
    "    W, words_voc = monskip.load(r\"C:\\Users\\Driss Debbagh\\Desktop\\Cours 3A\\Natural_Language_Processing\\SkipGram\\gridsearch\\10k\\negativerate_\" + str(negative_rate))\n",
    "    print(\"\\nNegative rate:\", negative_rate)\n",
    "    monskip.K_most_similar(\"president\", 10, words_voc, W)\n",
    "    print(\"\\n---------------------------------\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
